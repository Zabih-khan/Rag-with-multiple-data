{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel, RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "doc_loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = doc_loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "doc_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(doc_chunks, embedding_model)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "llm_model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "qa_chain = {\"context\": retriever, \"input\": RunnablePassthrough()} | qa_prompt | llm_model | output_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangSmith is a platform for building production-grade LLM applications. It allows for monitoring and evaluating applications to ensure quick and confident deployment. LangChain is not required for LangSmith to function.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain.invoke(\"what is langsmith\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Cost of LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Cost: $0.0003225000\n",
      "Completion Cost: $0.0000800000\n",
      "Total Cost: $0.0004025000\n",
      "Model's Response: LangSmith is a platform for building production-grade LLM applications. It allows users to closely monitor and evaluate their applications for quick and confident shipping. LangChain is not required for LangSmith to function.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is LangSmith?\"\n",
    "retrieved_context = retriever.invoke(query)\n",
    "\n",
    "formatted_prompt = qa_prompt.format(input=query, context=retrieved_context)\n",
    "model_response = llm_model.invoke(formatted_prompt)\n",
    "\n",
    "response_metadata = model_response.response_metadata\n",
    "completion_tokens = response_metadata['token_usage']['completion_tokens']\n",
    "prompt_tokens = response_metadata['token_usage']['prompt_tokens']\n",
    "\n",
    "price_per_1000_prompt_tokens = 0.0015\n",
    "price_per_1000_completion_tokens = 0.002\n",
    "\n",
    "prompt_cost = (prompt_tokens / 1000) * price_per_1000_prompt_tokens\n",
    "completion_cost = (completion_tokens / 1000) * price_per_1000_completion_tokens\n",
    "total_cost = prompt_cost + completion_cost\n",
    "\n",
    "print(f\"Prompt Cost: ${prompt_cost:.10f}\")\n",
    "print(f\"Completion Cost: ${completion_cost:.10f}\")\n",
    "print(f\"Total Cost: ${total_cost:.10f}\")\n",
    "\n",
    "print(f\"Model's Response: {model_response.content}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
